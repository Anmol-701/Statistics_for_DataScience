{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8b0b45-4d37-42db-9e17-8da6bf7929de",
   "metadata": {},
   "source": [
    "### Problem_1:What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5287c-d22d-4bd0-b39d-c1be6200d9e7",
   "metadata": {},
   "source": [
    "- Probability Mass Function (PMF)\n",
    "   - A PMF applies to discrete random variables, meaning they can take only isolated values. For instance, the number of times you flip a heads in 3 coin tosses (either 0, 1, 2, or 3) is a discrete random variable.\n",
    "   - The PMF assigns a probability to each possible value of the discrete random variable. The sum of all these probabilities must always equal 1.\n",
    "   - Visually, you can represent a PMF with a bar chart where the height of each bar corresponds to the probability of the value on the x-axis.\n",
    "- Example: Rolling a Die\n",
    "\n",
    "   - Imagine rolling a fair six-sided die. Each outcome (1, 2, 3, 4, 5, or 6) has an equal probability (1/6) of occurring.\n",
    "   \n",
    "- Probability Density Function (PDF)\n",
    "   - A PDF applies to continuous random variables, meaning they can take on any value within a specific range. For instance, the height of an adult human is a continuous random variable.\n",
    "   - Unlike PMF, a PDF doesn't directly provide the probability of a specific value. Instead, it describes the probability density over a range of values. To calculate the actual probability for a continuous variable, you need to integrate the PDF over that range.\n",
    "   - The integral of the PDF over the entire range of the variable must always equal 1.\n",
    "   - You can visualize a PDF using a smooth curve, where the area under the curve over a specific interval represents the probability of the variable falling within that interval.\n",
    "   \n",
    "- Example: Height of an Adult\n",
    "   - The height of an adult male might be modeled by a normal distribution with a certain mean and standard deviation. The PDF for this distribution describes the probability density of heights across the range, but it wouldn't give you the specific probability of someone being exactly 180 cm tall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adc49e-2607-45b7-851b-2be68b3c88ed",
   "metadata": {},
   "source": [
    "### Problem_2:What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3151df-20a6-412b-9633-a666cf185e4f",
   "metadata": {},
   "source": [
    "  - The Cumulative Distribution Function (CDF), denoted by F(x), is another crucial function in probability and statistics. It builds upon the concepts of PMF and PDF by providing the cumulative probability that a random variable will be less than or equal to a specific value (x).\n",
    "  \n",
    "- Understanding CDF:\n",
    "\n",
    "  - Applies to both discrete and continuous random variables.\n",
    "  - F(x) = P(X ≤ x), where X is the random variable and P represents probability.\n",
    "  - F(x) represents the probability that X takes a value less than or equal to x.\n",
    "  - The range of F(x) always falls between 0 and 1 (0 ≤ F(x) ≤ 1).\n",
    "    - F(-∞) = 0 (for continuous variables) or the minimum possible value for discrete variables.\n",
    "    - F(∞) = 1 (for continuous variables) or the maximum possible value for discrete variables.\n",
    "    \n",
    "- Example: Coin Toss\n",
    "- Imagine flipping a fair coin. Here, X can be either \"heads\" or \"tails.\"\n",
    "  - F(heads) = P(X ≤ heads) = 1 (since both heads and tails are possible outcomes less than or equal to heads).\n",
    "  - F(tails) = P(X ≤ tails) = 1 (all outcomes are less than or equal to tails in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af1acd-54fe-4567-956d-15e7309e3bc9",
   "metadata": {},
   "source": [
    "### Problem_3:What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa97d3a-7f8e-460d-b416-b2943b801d8f",
   "metadata": {},
   "source": [
    "- The normal distribution, also known as the bell curve, pops up in many real-world scenarios due to its property of describing naturally occurring variations around an average value. Here are some examples:\n",
    "\n",
    "  1. Height of Adults: When measuring the heights of a large population of adults, we wouldn't expect everyone to be exactly the same height. The normal distribution can effectively model this variation, with the mean representing the average height and the standard deviation capturing how spread out the heights are from the average.\n",
    "\n",
    "  2. Test Scores: Scores on standardized tests like SAT or ACT often follow a normal distribution. The mean represents the average score, and the standard deviation reflects how many students score above or below the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac31549-53ff-42dc-aaf1-48a375133303",
   "metadata": {},
   "source": [
    "### Problem_4:Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31df32-1685-4aac-9385-e071a88258dd",
   "metadata": {},
   "source": [
    "- The normal distribution, also called the Gaussian distribution, holds immense importance in statistics and various scientific fields due to several key reasons:\n",
    "\n",
    "  1. Universality in Modeling Natural Variation:  Many natural phenomena exhibit inherent variability around an average value. The normal distribution's bell-shaped curve effectively models this variation, making it a cornerstone for analyzing data across diverse domains. From heights and weights of individuals to test scores and errors in measurements, the normal distribution provides a foundational framework.\n",
    "\n",
    "  2. Central Limit Theorem: This crucial theorem states that under certain conditions, the sampling distribution of the mean (average) of repeated random samples from a population, regardless of the original population's distribution (as long as it's not highly skewed), tends towards a normal distribution as the sample size increases. This allows us to apply statistical methods assuming normality even when the underlying data isn't perfectly normal, especially for larger samples.\n",
    "  \n",
    "- Real-Life Examples of Normal Distribution:\n",
    "  1. Biology: Distribution of heights, weights, and lifespans within a population.\n",
    "  2. Finance: Stock market returns, fluctuations in currency exchange rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590521f-5de5-47cb-be58-8f165a44dd52",
   "metadata": {},
   "source": [
    "### Problem_5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f0d8e1-d4fd-4036-89d7-27b52192693f",
   "metadata": {},
   "source": [
    "- The Bernoulli distribution describes the probability of a single trial with only two possible outcomes: success (often denoted by 1) and failure (often denoted by 0). It's a fundamental building block for understanding binomial distributions.\n",
    "\n",
    "- Example: Flipping a fair coin is a classic Bernoulli trial. There are two outcomes: heads (success) or tails (failure). The probability of success (heads) is denoted by p (usually between 0 and 1), and the probability of failure (tails) is 1 - p.\n",
    "\n",
    "- Key Points of Bernoulli Distribution:\n",
    "  1. Deals with one trial, not repeated trials.\n",
    "  2. Only has two possible outcomes: success and failure.\n",
    "  3. Defined by a single parameter p: the probability of success.\n",
    "  4. The probability of failure is simply 1 - p.\n",
    "  \n",
    "  \n",
    "- Binomial Distribution vs. Bernoulli Distribution \n",
    "\n",
    "  The binomial distribution builds upon the concept of Bernoulli trials. It describes the probability of getting a specific number of successes in a series of independent Bernoulli trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2047ad-321b-480e-8e49-dfc7e044ebe8",
   "metadata": {},
   "source": [
    "### Problem_6:Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045df85-074c-4dfb-9dc9-29452999e3cd",
   "metadata": {},
   "source": [
    "- The dataset is normally distributed with a mean of 50 and a standard deviation of 10, we can use the cumulative distribution function (CDF) to calculate the probability of a randomly selected observation being greater than 60.\n",
    "\n",
    "- The CDF of the normal distribution, denoted by norm.cdf(x, mean, std_dev) in Python's scipy.stats library, gives the probability that a random variable will be less than or equal to a certain value (x) within the distribution defined by the mean and standard deviation.\n",
    "\n",
    "- In this case, we're interested in the probability of values greater than 60. Since the normal distribution is symmetrical, the probability of a value being less than -60 (60 below the mean) is equal to the probability of a value being greater than 60 (60 above the mean). Therefore, we can calculate the probability of a value greater than 60 by finding the area to the right of 60 under the normal curve and subtracting it from 1 (the total area under the curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da04496-48c1-494e-9bed-9e5c3fcc006c",
   "metadata": {},
   "source": [
    "### Problem_7:Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3212887-b8a1-4394-a9d2-d165e0393c20",
   "metadata": {},
   "source": [
    "- A uniform distribution describes a scenario where all possible outcomes within a specific range are equally likely. It's a fundamental concept in probability, representing situations where chance alone dictates the results, and no outcome has a preferential advantage.\n",
    "- Imagine a fair roulette wheel with 38 numbered slots (including 0 and 00). Each slot has an equal chance of landing on the ball when spun. In this case, the landing position of the ball follows a uniform distribution. Every number from 0 to 37 has an equal probability (1/38) of being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c4c84-4aa7-4769-a1db-49337e0abe82",
   "metadata": {},
   "source": [
    "### Problem_8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed5033-c2f8-426c-a101-a3a783db7abc",
   "metadata": {},
   "source": [
    "- A z-score, also known as a standard score, is a statistical measurement that tells you how many standard deviations a specific point (data value) is away from the mean (average) of a dataset assuming it follows a normal distribution. In simpler terms, it indicates how far a particular value falls from the average value when expressed in units of standard deviation.\n",
    "\n",
    "- Importance of Z-scores:\n",
    "\n",
    "  - Z-scores hold significant value in statistics for several reasons:\n",
    "\n",
    "  1. Standardization: Z-scores allow for comparison between data points from different datasets, even if they were measured on different scales. By expressing data in terms of standard deviations from the mean, z-scores remove the influence of the original measurement units, enabling comparisons across datasets.\n",
    "  2. Identifying Outliers: Z-scores help identify outliers in a dataset. Values with very high negative or positive z-scores (typically beyond +/- 3 standard deviations) are considered outliers and may warrant further investigation as they deviate significantly from the typical pattern.\n",
    "  3. Hypothesis Testing: Z-scores play a role in various statistical tests that rely on the assumption of normality. By transforming data into z-scores, we can utilize established statistical tables and procedures for hypothesis testing, allowing us to draw inferences about the population from which the data came."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fc96c-3a59-41f2-af89-afeaaf1930ff",
   "metadata": {},
   "source": [
    "### Problem_9:What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f05b95-a703-4455-8789-80255748cafc",
   "metadata": {},
   "source": [
    "- The Central Limit Theorem (CLT) is a fundamental principle in probability and statistics. It describes the behavior of the sampling distribution of the mean under certain conditions. Here's a breakdown:\n",
    "\n",
    "- Core Idea:\n",
    "  - Imagine taking random samples of a certain size (n) from a population, regardless of the original population's distribution (as long as it's not highly skewed).\n",
    "  - The CLT states that as the sample size (n) increases, the distribution of the means of these samples (sampling distribution of the mean) will tend towards a normal distribution. This holds true even if the original population's distribution isn't normal itself.\n",
    "\n",
    "- Significance of the Central Limit Theorem:\n",
    "- The CLT holds immense importance in statistics for several reasons:\n",
    "\n",
    "  1. Wide Applicability: It allows us to apply statistical methods that rely on normality (like hypothesis testing, confidence intervals) to data that may not be perfectly normal, especially for larger samples. This is incredibly useful in real-world scenarios where perfect normality is often rare.\n",
    "  2. Confidence Interval Construction: We can construct confidence intervals around the population mean even for non-normal data using the CLT. This allows us to estimate the population mean with a certain level of confidence.\n",
    "  3. Understanding Sampling Error: The CLT helps us understand how much the sample mean might deviate from the population mean due to sampling variability. By knowing the expected distribution of sample means, we can quantify this sampling error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac5b73-08c1-4b82-8a54-a7cb79eb101d",
   "metadata": {},
   "source": [
    "### Problem_10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1c300-8a11-43ff-ad16-6e791eafece3",
   "metadata": {},
   "source": [
    "- The Central Limit Theorem (CLT) is a powerful tool in statistics, but it relies on certain assumptions for its validity. Here are the key assumptions of the CLT:\n",
    "\n",
    "  1. Random Samples: The samples must be drawn randomly and independently from the population. This means each sample has an equal chance of being selected, and the selection of one sample doesn't influence the selection of others. Techniques like biased sampling or sampling without replacement can invalidate the CLT.\n",
    "  2. Sufficient Sample Size:  There's no universally agreed-upon minimum sample size for the CLT to hold true. However, a general rule suggests that the larger the sample size, the better the approximation of the sampling distribution to a normal distribution. In practice, sample sizes greater than 30 are often considered sufficient for the CLT to apply effectively.\n",
    "  3. Finite Variance: The population from which you're sampling should have a finite variance.  Variance measures how spread out the data is from the mean. Distributions with infinite variance, like the Cauchy distribution, violate this assumption and the CLT wouldn't be applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48893e1f-7420-4969-9cff-a38d10a80b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
